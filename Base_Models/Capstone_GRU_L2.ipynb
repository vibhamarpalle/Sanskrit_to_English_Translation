{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFtT4J2lDstS",
        "outputId": "708a06a3-a9f3-4c7d-b719-6eba0c323352"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch==2.2.0 torchtext==0.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po5ufoF4HPTB",
        "outputId": "1a04bc29-95da-43fe-94e8-090bf665cd95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 2.2.0\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3\n",
            "Location: C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
            "Required-by: torchdata, torchtext\n",
            "---\n",
            "Name: torchtext\n",
            "Version: 0.17.0\n",
            "Summary: Text utilities, models, transforms, and datasets for PyTorch.\n",
            "Home-page: https://github.com/pytorch/text\n",
            "Author: PyTorch Text Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD\n",
            "Location: C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\n",
            "Requires: numpy, requests, torch, torchdata, tqdm\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show torch torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlrd in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install xlrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gqo1pG-ntgDP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 1. Load and Prepare Your Dataset\n",
        "data_path = r'C:\\Users\\Dell\\Documents\\Capstone\\Working\\dict.csv'  # Replace with your dataset path\n",
        "df = pd.read_csv(data_path, encoding = \"utf-8\")\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "sanskrit_sentences = df['Sanskrit'].tolist()  # Column name containing Sanskrit sentences\n",
        "english_sentences = df['English'].tolist()  # Column name containing English sentences\n",
        "\n",
        "# 2. Tokenize and Build Vocabulary\n",
        "sanskrit_tokenizer = get_tokenizer('basic_english')  # Customize tokenizer if needed\n",
        "english_tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data, tokenizer):\n",
        "    for text in data:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "sanskrit_vocab = build_vocab_from_iterator(yield_tokens(sanskrit_sentences, sanskrit_tokenizer), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "english_vocab = build_vocab_from_iterator(yield_tokens(english_sentences, english_tokenizer), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "\n",
        "sanskrit_vocab.set_default_index(sanskrit_vocab[\"<unk>\"])\n",
        "english_vocab.set_default_index(english_vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSck7oPa-_OT",
        "outputId": "c9fe4b56-2867-40ca-a4b2-41c6399730ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      English Sanskrit\n",
            "0           I     अहम्\n",
            "1          me     माम्\n",
            "2         you    त्वम्\n",
            "3          go     गच्छ\n",
            "4        went  अगच्छत्\n",
            "...       ...      ...\n",
            "1604   breath   श्वासः\n",
            "1605  breathe   श्वसति\n",
            "1606    brick      इटः\n",
            "1607   bridge    पुलम्\n",
            "1608    brief    संक्ष\n",
            "\n",
            "[1609 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gAuZBf9e5Icq"
      },
      "outputs": [],
      "source": [
        "def process_text(text, tokenizer, vocab):\n",
        "    return [vocab[\"<bos>\"]] + [vocab[token] for token in tokenizer(text)] + [vocab[\"<eos>\"]]\n",
        "\n",
        "sanskrit_data = [process_text(sentence, sanskrit_tokenizer, sanskrit_vocab) for sentence in sanskrit_sentences]\n",
        "english_data = [process_text(sentence, english_tokenizer, english_vocab) for sentence in english_sentences]\n",
        "\n",
        "# Padding sequences\n",
        "sanskrit_data = pad_sequence([torch.tensor(seq) for seq in sanskrit_data], batch_first=True, padding_value=sanskrit_vocab[\"<pad>\"])\n",
        "english_data = pad_sequence([torch.tensor(seq) for seq in english_data], batch_first=True, padding_value=english_vocab[\"<pad>\"])\n",
        "\n",
        "# 3. Create Dataset and DataLoader\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.tgt_data[idx]\n",
        "\n",
        "train_dataset = TranslationDataset(sanskrit_data, english_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hAM0ZQui5PvY"
      },
      "outputs": [],
      "source": [
        "# 4. Define GRU-based Seq2Seq Model\n",
        "class EncoderGRU(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers):\n",
        "        super(EncoderGRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)  # embedded: [batch_size, src_len, emb_dim]\n",
        "        outputs, hidden = self.rnn(embedded)  # outputs: [batch_size, src_len, hid_dim], hidden: [n_layers, batch_size, hid_dim]\n",
        "        return hidden\n",
        "\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers):\n",
        "        super(DecoderGRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, tgt, hidden):\n",
        "        embedded = self.embedding(tgt)  # embedded: [batch_size, tgt_len, emb_dim]\n",
        "        outputs, hidden = self.rnn(embedded, hidden)  # outputs: [batch_size, tgt_len, hid_dim], hidden: [n_layers, batch_size, hid_dim]\n",
        "        predictions = self.fc_out(outputs)  # predictions: [batch_size, tgt_len, output_dim]\n",
        "        return predictions, hidden\n",
        "\n",
        "class Seq2SeqGRU(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2SeqGRU, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        hidden = self.encoder(src)  # hidden: [n_layers, batch_size, hid_dim]\n",
        "        outputs, _ = self.decoder(tgt, hidden)  # outputs: [batch_size, tgt_len, output_dim]\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k19P5XZw5TYN",
        "outputId": "d254d4bf-685b-4a96-b3a7-ec29a3a08618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 3.8338\n",
            "Epoch 2/20, Loss: 3.7985\n",
            "Epoch 3/20, Loss: 3.4379\n",
            "Epoch 4/20, Loss: 2.3905\n",
            "Epoch 5/20, Loss: 0.8677\n",
            "Epoch 6/20, Loss: 0.3440\n",
            "Epoch 7/20, Loss: 0.3194\n",
            "Epoch 8/20, Loss: 0.2575\n",
            "Epoch 9/20, Loss: 0.2374\n",
            "Epoch 10/20, Loss: 0.2193\n",
            "Epoch 11/20, Loss: 0.2217\n",
            "Epoch 12/20, Loss: 0.2052\n",
            "Epoch 13/20, Loss: 0.1916\n",
            "Epoch 14/20, Loss: 0.1627\n",
            "Epoch 15/20, Loss: 0.1577\n",
            "Epoch 16/20, Loss: 0.1553\n",
            "Epoch 17/20, Loss: 0.1511\n",
            "Epoch 18/20, Loss: 0.1416\n",
            "Epoch 19/20, Loss: 0.1369\n",
            "Epoch 20/20, Loss: 0.1400\n"
          ]
        }
      ],
      "source": [
        "# 5. Initialize and Train the Model\n",
        "INPUT_DIM = len(sanskrit_vocab)\n",
        "OUTPUT_DIM = len(english_vocab)\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "N_EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "encoder_gru = EncoderGRU(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS)\n",
        "decoder_gru = DecoderGRU(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS)\n",
        "model_gru = Seq2SeqGRU(encoder_gru, decoder_gru)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[\"<pad>\"])\n",
        "optimizer = torch.optim.Adam(model_gru.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model_gru.train()\n",
        "    epoch_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model_gru(src, tgt[:, :-1])  # Exclude the last token for the target\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)  # Exclude the first token for the target\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{N_EPOCHS}, Loss: {epoch_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "75wVJ6neXn1C"
      },
      "outputs": [],
      "source": [
        "state = {\n",
        "    \"epoch\": 20,\n",
        "    \"model\": model_gru,\n",
        "    \"model_state_dict\": model_gru.state_dict(),\n",
        "    \"optimizer\": optimizer.state_dict()\n",
        "}\n",
        "torch.save(state, \"temp.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GekESyocDNa0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model_gru.eval()\n",
        "\n",
        "def predict_translation(model, input_sentence, sanskrit_tokenizer, sanskrit_vocab, english_vocab, max_len=50):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Preprocess the input sentence\n",
        "    tokens = sanskrit_tokenizer(input_sentence.lower())\n",
        "    input_indices = [sanskrit_vocab[\"<bos>\"]] + [sanskrit_vocab[token] for token in tokens] + [sanskrit_vocab[\"<eos>\"]]\n",
        "    input_tensor = torch.tensor(input_indices).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Encode the input sentence\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(input_tensor)\n",
        "\n",
        "    # Initialize the target sequence with the <bos> token\n",
        "    tgt_indices = [english_vocab[\"<bos>\"]]\n",
        "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Prepare to store the predicted sentence\n",
        "    predicted_sentence = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Decode the current token\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(tgt_tensor, hidden)\n",
        "\n",
        "        # Get the predicted next token\n",
        "        predicted_token_index = output.argmax(2)[:, -1].item()\n",
        "        predicted_sentence.append(predicted_token_index)\n",
        "\n",
        "        # If <eos> token is generated, stop the prediction loop\n",
        "        if predicted_token_index == english_vocab[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "        # Update the target sequence with the predicted token\n",
        "        tgt_tensor = torch.cat((tgt_tensor, torch.tensor([[predicted_token_index]])), dim=1)\n",
        "\n",
        "    # Convert predicted indices back to words\n",
        "    translated_words = [english_vocab.get_itos()[idx] for idx in predicted_sentence]\n",
        "\n",
        "    # Remove <eos> if it's in the translated words\n",
        "    if \"<eos>\" in translated_words:\n",
        "        translated_words.remove(\"<eos>\")\n",
        "\n",
        "    return ' '.join(translated_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KrHuQOrEEHH",
        "outputId": "fd4edd12-1c5f-4775-a88a-3d176fe16126"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'predict_translation' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[0;32m      2\u001b[0m input_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mफलम\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m translation \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_translation\u001b[49m(model_gru, input_sentence, sanskrit_tokenizer, sanskrit_vocab, english_vocab)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'predict_translation' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example\n",
        "input_sentence = \"फलम\"\n",
        "translation = predict_translation(model_gru, input_sentence, sanskrit_tokenizer, sanskrit_vocab, english_vocab)\n",
        "print(f\"Translated: {translation}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "st.write(\"Sanskrit to English Translation\")\n",
        "input_sentence = st.text_input(\"Word to be translated\")\n",
        "translation = predict_translation(model_gru, input_sentence, sanskrit_tokenizer, sanskrit_vocab, english_vocab)\n",
        "st.write(f\"Translated: {translation}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
